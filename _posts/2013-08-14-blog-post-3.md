---
title: 'KNN to clasify the IRIS dataset, using numpy.'
date: 2022-08-28
permalink: /posts/2022/07/python_euclid_extended/
tags:
  - Python
  - Machine Learning
---

The [IRIS dataset](https://archive.ics.uci.edu/ml/datasets/iris) is a dataset which consists of measurements taken from various flowers such as
the petal length and width, as well as the species of that flower. It can be downloaded from [here](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/) as a .data file, and opened as a numpy array in python like so:

```python
# Imports + setting seed
import numpy as np
import pandas as pd
import random as r
r.seed(1)

# Load the data
data =  pd.read_csv('iris.data', sep=",")
data = data.to_numpy()  
```

Note that whilst the loading uses pandas, the actual code for creating the AI will be done exclusively with numpy and python - so, without further ado, what AI will we be creating?

Today, we will be building a K-nearest-neighbours (KNN) algorithm. The core idea of KNN is "if 2 datapoints look similar, they are similar" and we will be taking some time to expand upon this mathematically. To start, we need to somehow quantify this similarity mathematically. One way to do this is with a [distance function](https://en.wikipedia.org/wiki/Metric_(mathematics)). Distance functions are a way of assigning a real number to two elements of a set. Intuitively, they are a way of measuring how similar 2 elements of a set are - making them ideal to try classify our flowers, as we can reasonably expect flowers of the same species to be similar to each other, and flowers of the same species to be different to each other. For now, let's just use the euclidean distance, defined mathematically for vectors \\(A,B\\) as:

\\[ \sqrt{\sum^n_{i=1}(a_i - b_i)^2}, \text{ for } a_{1,...,n},b_{1,...,n} \in A,B. \\]

And in python as:

```python
# Euclidean distance
def euc_dist(A,B):
    return np.sqrt(np.sum(np.square(A-B)))
```

Now we've done this, we can work towards using this distance function within KNN. Let's say there is some flower that we want to classify using KNN, which in the code we will refer to as test_images[j]. Then, we can find the distance between this point and every flower we already have classified (refered to in the code as train_images) in order to find how "similar" they are, according to our distance metric. In python, this is quite a quick implementation:

```python
# Find distance to each point.
distances = list()
for i in range(train_images.shape[0]):
    distances.append(euc_dist(test_images[j],train_images[i]))
```

From here, we need to find the "most similar" items to test_images[j]. When using a distance function, the closer the distance is to 0 the more "similar" the two items are. Hence, we see that by simply sorting our distances list and noting that distance functions cannot assign negative distances, that we can find how close each flower is to the one we want to classify. Note here that train_labels in the code is simply the names of the flowers that we already have classified:

```python
# Sort distances
sort = sorted(distances)

# Find nearest n
closest = list()
for i in range(n):
    closest.append(train_labels[distances.index(sort[i])])
```

Finally, as we have a list of the "nearest" flowers to our unknown flower, we can find the most commonly occuring name within these nearest flowers, and using that, return this most common name as a prediction of what our unknown flower is (again noting that the reason we do this is because of the big idea of KNN - "things that look similar are similar"):

```python
# Find mode of list, and save it as a prediction
preds.append(max(set(closest), key=closest.count))
```

By wrapping all of this into a loop, we can check the entire collection of flowers we want to classify at once:

```python
def KNN(test_images,train_images,test_labels,train_labels,n=5):
    """
    Performs the K nearest neighbours
    algorithm on the test + train data.
    """
    preds = list()
    for j in range(test_images.shape[0]):
        
    # Find distance to each point.
    distances = list()
    for i in range(train_images.shape[0]):
        distances.append(euc_dist(test_images[j],train_images[i]))

    # Sort distances
    sort = sorted(distances)

    # Find nearest n
    closest = list()
    for i in range(n):
        closest.append(train_labels[distances.index(sort[i])])

    # Find mode of list, and save it as a prediction
    preds.append(max(set(closest), key=closest.count))
        
    return preds
```

Now that we have a KNN implementation, we can create a simple accuracy function. Mathematically, we can quantify the accuracy as:

\\[ \frac{\text{Number correct}}{\text{Number predicted}} \\]

And, in python:

```python
def accuracy(preds,labels):
    """
    Finds the accuracy of KNN given
    some known truth values.
    """
    count = 0
    for i in range(len(preds)):
        if preds[i]==labels[i]:
            count = count + 1
    return count/len(preds)
```

Putting all of this together, we find our full code to classify the IRIS dataset using numpy and python to be:

```python
# Imports + setting seed
import numpy as np
import pandas as pd
import random as r
r.seed(1)

# Load the data
data =  pd.read_csv('iris.data', sep=",")
data = data.to_numpy()

# Shuffle the data
temp = list()
for i in range(149):
    temp.append(data[i,:])
r.shuffle(temp)
data = np.asarray(temp)

# Split into test,train
train_images = data[0:125,0:3]
train_labels = data[0:125,4]
test_images = data[125:149,0:3]
test_labels = data[125:149,4]

# Euclidean distance
def euc_dist(A,B):
    return np.sqrt(np.sum(np.square(A-B)))

def KNN(test_images,train_images,test_labels,train_labels,n=5):
    """
    Performs the K nearest neighbours
    algorithm on the test + train data.
    """
    preds = list()
    for j in range(test_images.shape[0]):
        
        # Find distance to each point.
        distances = list()
        for i in range(train_images.shape[0]):
            distances.append(euc_dist(test_images[j],train_images[i]))

        # Sort distances
        sort = sorted(distances)

        # Find nearest n
        closest = list()
        for i in range(n):
            closest.append(train_labels[distances.index(sort[i])])

        # Find mode of list, and save it as a prediction
        preds.append(max(set(closest), key=closest.count))
        
    return preds

def accuracy(preds,labels):
    """
    Finds the accuracy of KNN given
    some known truth values.
    """
    count = 0
    for i in range(len(preds)):
        if preds[i]==labels[i]:
            count = count + 1
    return count/len(preds)

preds = KNN(test_images,train_images,test_labels,train_labels)
accuracy(preds,test_labels)
```
Running this we see that our accuracy is \\( 95.8% \\) - a great result for a program we coded in less than 100 lines of code!
